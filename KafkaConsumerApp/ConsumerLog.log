01:24:37.106 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-CustomConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = CustomConsumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.sre.teaching.kafka.KafkaConsumerApp.deserializer.KafkaConsumerCustomDeserializer

01:24:37.730 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.3.2
01:24:37.731 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: b66af662e61082cb
01:24:37.731 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1677527677727
01:24:37.735 [main] INFO  o.a.k.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Subscribed to topic(s): customdatatopic
01:24:38.107 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-0 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:24:38.108 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-1 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:24:38.108 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-2 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:24:38.112 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Cluster ID: IzSEXMkoSx2XgxuAFsUc0g
01:24:38.114 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null)
01:24:38.124 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Found no committed offset for partition customdatatopic-0
01:24:38.124 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Found no committed offset for partition customdatatopic-1
01:24:38.125 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Found no committed offset for partition customdatatopic-2
01:24:38.125 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Found no committed offset for partition customdatatopic-3
01:24:38.128 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] (Re-)joining group
01:24:38.141 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Request joining group due to: need to re-join with the given member-id: consumer-CustomConsumer-1-cafc15d9-7f81-48a7-ba3a-22916a944f62
01:24:38.141 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
01:24:38.141 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] (Re-)joining group
01:24:38.144 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-CustomConsumer-1-cafc15d9-7f81-48a7-ba3a-22916a944f62', protocol='range'}
01:24:38.145 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Finished assignment for group at generation 1: {consumer-CustomConsumer-1-cafc15d9-7f81-48a7-ba3a-22916a944f62=Assignment(partitions=[customdatatopic-0, customdatatopic-1, customdatatopic-2])}
01:24:38.151 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-CustomConsumer-1-cafc15d9-7f81-48a7-ba3a-22916a944f62', protocol='range'}
01:24:38.152 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Notifying assignor about the new Assignment(partitions=[customdatatopic-0, customdatatopic-1, customdatatopic-2])
01:24:38.153 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Adding newly assigned partitions: customdatatopic-0, customdatatopic-1, customdatatopic-2
01:24:38.154 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-0
01:24:38.155 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-1
01:24:38.155 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-2
01:24:38.180 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9094 (id: 2 rack: null)], epoch=0}}.
01:24:38.187 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9093 (id: 1 rack: null)], epoch=0}}.
01:24:38.188 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.
01:24:38.295 [main] ERROR c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Failed to deserialize. Reason : Cannot construct instance of `com.sre.teaching.kafka.KafkaConsumerApp.datamodel.CustomData` (no Creators, like default constructor, exist): cannot deserialize from Object value (no delegate- or property-based Creator)
 at [Source: (byte[])"{"id":"2","name":"swami","salary":65000.0,"yearsOfExp":4}"; line: 1, column: 2]
01:24:38.297 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:24:38.297 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - null
01:24:38.302 [main] ERROR c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Failed to deserialize. Reason : Cannot construct instance of `com.sre.teaching.kafka.KafkaConsumerApp.datamodel.CustomData` (no Creators, like default constructor, exist): cannot deserialize from Object value (no delegate- or property-based Creator)
 at [Source: (byte[])"{"id":"1","name":"mahi","salary":45000.0,"yearsOfExp":2}"; line: 1, column: 2]
01:24:38.302 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:24:38.302 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - null
01:26:26.615 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-CustomConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = CustomConsumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.sre.teaching.kafka.KafkaConsumerApp.deserializer.KafkaConsumerCustomDeserializer

01:26:27.365 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.3.2
01:26:27.367 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: b66af662e61082cb
01:26:27.367 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1677527787364
01:26:27.370 [main] INFO  o.a.k.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Subscribed to topic(s): customdatatopic
01:26:27.766 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-0 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:26:27.766 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-1 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:26:27.766 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-2 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:26:27.769 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Cluster ID: IzSEXMkoSx2XgxuAFsUc0g
01:26:27.771 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null)
01:26:27.781 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Found no committed offset for partition customdatatopic-3
01:26:27.785 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] (Re-)joining group
01:26:27.799 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Request joining group due to: need to re-join with the given member-id: consumer-CustomConsumer-1-66d0df5f-0e10-44cb-8e48-cfcf5544f65c
01:26:27.799 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
01:26:27.799 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] (Re-)joining group
01:26:27.802 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Successfully joined group with generation Generation{generationId=3, memberId='consumer-CustomConsumer-1-66d0df5f-0e10-44cb-8e48-cfcf5544f65c', protocol='range'}
01:26:27.804 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Finished assignment for group at generation 3: {consumer-CustomConsumer-1-66d0df5f-0e10-44cb-8e48-cfcf5544f65c=Assignment(partitions=[customdatatopic-0, customdatatopic-1, customdatatopic-2])}
01:26:27.810 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Successfully synced group in generation Generation{generationId=3, memberId='consumer-CustomConsumer-1-66d0df5f-0e10-44cb-8e48-cfcf5544f65c', protocol='range'}
01:26:27.810 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Notifying assignor about the new Assignment(partitions=[customdatatopic-0, customdatatopic-1, customdatatopic-2])
01:26:27.813 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Adding newly assigned partitions: customdatatopic-0, customdatatopic-1, customdatatopic-2
01:26:27.814 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-0
01:26:27.814 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-1
01:26:27.814 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-2
01:26:27.838 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.
01:26:27.844 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9094 (id: 2 rack: null)], epoch=0}}.
01:26:27.854 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9093 (id: 1 rack: null)], epoch=0}}.
01:26:27.943 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:26:27.946 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - CustomData{ID=1,Name='mahi',Salary=45000.0,Experience=2}
01:26:27.963 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:26:27.963 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - CustomData{ID=2,Name='swami',Salary=65000.0,Experience=4}
01:29:23.562 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:29:23.563 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - CustomData{ID=3,Name='anshul',Salary=89000.0,Experience=10}
01:30:11.132 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-CustomConsumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = CustomConsumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.sre.teaching.kafka.KafkaConsumerApp.deserializer.KafkaConsumerCustomDeserializer

01:30:11.730 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.3.2
01:30:11.731 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: b66af662e61082cb
01:30:11.732 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1677528011728
01:30:11.735 [main] INFO  o.a.k.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Subscribed to topic(s): customdatatopic
01:30:12.122 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-0 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:30:12.122 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-1 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:30:12.122 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting the last seen epoch of partition customdatatopic-2 to 0 since the associated topicId changed from null to qf0b57GoRE2DE5r2_t9fFQ
01:30:12.127 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Cluster ID: IzSEXMkoSx2XgxuAFsUc0g
01:30:12.129 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null)
01:30:12.139 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Found no committed offset for partition customdatatopic-3
01:30:12.143 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] (Re-)joining group
01:30:12.156 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Request joining group due to: need to re-join with the given member-id: consumer-CustomConsumer-1-4e250032-e171-4df7-8003-ebbaabf1c44a
01:30:12.157 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
01:30:12.157 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] (Re-)joining group
01:30:49.458 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Successfully joined group with generation Generation{generationId=4, memberId='consumer-CustomConsumer-1-4e250032-e171-4df7-8003-ebbaabf1c44a', protocol='range'}
01:30:49.459 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Finished assignment for group at generation 4: {consumer-CustomConsumer-1-4e250032-e171-4df7-8003-ebbaabf1c44a=Assignment(partitions=[customdatatopic-0, customdatatopic-1, customdatatopic-2])}
01:30:49.464 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Successfully synced group in generation Generation{generationId=4, memberId='consumer-CustomConsumer-1-4e250032-e171-4df7-8003-ebbaabf1c44a', protocol='range'}
01:30:49.464 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Notifying assignor about the new Assignment(partitions=[customdatatopic-0, customdatatopic-1, customdatatopic-2])
01:30:49.466 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Adding newly assigned partitions: customdatatopic-0, customdatatopic-1, customdatatopic-2
01:30:49.467 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-0
01:30:49.468 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-1
01:30:49.468 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Seeking to earliest offset of partition customdatatopic-2
01:30:49.489 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9094 (id: 2 rack: null)], epoch=0}}.
01:30:49.495 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.
01:30:49.495 [main] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-CustomConsumer-1, groupId=CustomConsumer] Resetting offset for partition customdatatopic-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9093 (id: 1 rack: null)], epoch=0}}.
01:30:49.580 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:30:49.584 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - CustomData{ID=1,Name='mahi',Salary=45000.0,Experience=2}
01:30:49.586 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:30:49.587 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - CustomData{ID=2,Name='swami',Salary=65000.0,Experience=4}
01:30:49.587 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - Successfully Deserialized to Object
01:30:49.587 [main] INFO  c.s.t.k.K.d.KafkaConsumerCustomDeserializer - CustomData{ID=3,Name='anshul',Salary=89000.0,Experience=10}
